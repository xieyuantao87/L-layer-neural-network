{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "   # initial parameters with random number\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(shape=(n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros(shape=(n_y, 1))\n",
    "   \n",
    "    #double check the shape of matrixs are right\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,  \"b1\": b1,  \"W2\": W2, \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    layer_dims[0] is the number of input features, layer_dims[-1] is dimension of output\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    # number of layers in the network\n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "       \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A)+b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoild function \n",
    "def sigmoid(Z):\n",
    "    A=1 / (1 + np.exp(-Z))\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rectified linear unit \n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                \n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n",
    "        caches.append(cache)        \n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    \n",
    "    AL, cache = linear_activation_forward(A,  parameters['W' + str(L)],  parameters['b' + str(L)],  activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "      \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    \n",
    "    dW = np.dot(dZ, cache[0].T)/m\n",
    "    db =np.sum(dZ, axis=1, keepdims=True)/ m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    #assert (isinstance(db, float))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    s = 1/(1+1/np.exp(Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "   \n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(Z, cache):\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)  \n",
    "    elif activation == \"sigmoid\": \n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "   \n",
    "    dAL = dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "   \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "   \n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, current_cache[1]), \n",
    "                                                                                       current_cache[0])\n",
    " \n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads[\"dA\" + str(l + 1)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = linear_backward(sigmoid_backward(dAL, current_cache[1]),\n",
    "                                                                                                       current_cache[0])\n",
    "   \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): #lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        costs.append(cost)        \n",
    "        # Print the cost every 100 training example\n",
    "        #if print_cost and i % 100 == 0:\n",
    "        #   print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        #if print_cost and i % 100 == 0:\n",
    "         #   costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations steps')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer=load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871   ...            17.33           184.60      2019.0   \n",
       "1                 0.05667   ...            23.41           158.80      1956.0   \n",
       "2                 0.05999   ...            25.53           152.50      1709.0   \n",
       "3                 0.09744   ...            26.50            98.87       567.7   \n",
       "4                 0.05883   ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890     0.0  \n",
       "1          0.2750                  0.08902     0.0  \n",
       "2          0.3613                  0.08758     0.0  \n",
       "3          0.6638                  0.17300     0.0  \n",
       "4          0.2364                  0.07678     0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = np.append(cancer['feature_names'], ['target'])\n",
    "data_n_target = np.vstack((cancer['data'].T, cancer['target'])).T\n",
    "    \n",
    "df=pd.DataFrame(columns=columns, data=data_n_target)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_data=df[cancer['feature_names']]\n",
    "y_data=df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0)\n",
    "y_train=y_train.reshape((1, y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_dims=[len(cancer['feature_names']), 20, 15, 10, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in multiply\n",
      "  app.launch_new_instance()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in maximum\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmYl52JjZmYl52JjZmYl9/8BFGmO6mi1HgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1465ecd5668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]]),\n",
       " 'W2': array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]]),\n",
       " 'W3': array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "          nan,  nan,  nan,  nan]]),\n",
       " 'W4': array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n",
       "        [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]]),\n",
       " 'W5': array([[ nan,  nan,  nan,  nan,  nan]]),\n",
       " 'b1': array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]]),\n",
       " 'b2': array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]]),\n",
       " 'b3': array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]]),\n",
       " 'b4': array([[ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan],\n",
       "        [ nan]]),\n",
       " 'b5': array([[ nan]])}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_layer_model(X_train.T, y_train.T, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
